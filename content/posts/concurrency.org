#+title: The best definition of concurrency
#+date: <2025-09-19 Fri>

#+begin_quote
If I had more time, I would have written a shorter letter.
-- [[https://quoteinvestigator.com/2012/04/28/shorter-letter/][Blaise Pascal]]
#+end_quote

In my first few years of learning programming,
I only had to deal with sequential programs.
I disregarded the topics of multi-threading and async/await
whenever I came across them,
thinking they were merely another way in which programmers can waste time
to do premature performance optimizations.
But I was wrong, concurrency is not just about gaining a 4x speed-up using your multi-core processor,
it has many more uses (e.g. modeling independent entities from the real world),
and sometimes is simply unavoidable (e.g. in network applications).

Eventually I was finally persuaded to learn about concurrency
and, as programmers often do,
I started googling for "quick guides", thinking it will take me a few hours tops.
Sure.
It wasn't until two years later when I finally got a solid grasp on it.

The more I read explanations and analogies online,
the more confused I was.
How is concurrency different from parallelism?
How is it related to threads and processes?
Is concurrency about time scheduling?

Then I randomly stumbled upon [[https://slikts.github.io/concurrency-glossary/][this glossary of concurrency terms]]
(from here on, "The Glossary")
and found it absolutely enlightening.
It had cleared up the mess in my head
and helped me build a coherent mental model
of all things concurrency.
Now I think that most analogies and definitions found online
are imprecise, confusing, and sometimes even contradictory.

In this post I'll vent about my frustrating experience of learning concurrency
(which I dare to guess is pretty common),
and will try to guide you through the definitions from The Glossary,
which, for simplicity, I'll be calling "the correct definitions" in this article.[fn:1]

* A wrong understanding concurrency

My previous (*WRONG*) understanding of concurrency went something like this:
- Parallelism is when you literally execute more than one task at the same time
  (e.g. on multiple cores of a processor)
- Concurrency is when you execute tasks such that they
  *appear* to be executed at the same time,
  while not necessarily being executed in parallel (e.g. on a single core);

Clearly, by this definition, concurrency can be seen as a union of two techniques:
- parallel execution
- time slicing (preemption)

This led me to reason that concurrency implies parallelism but not vice versa,
i.e. that concurrency is a more general description of the method of execution,
whereas parallelism is a more specific description of the method of execution.[fn:2]

This understanding of concurrency seems to be quite popular,
judging by the fact that it is [[https://stackoverflow.com/a/1050257][the top answer in a StackOverflow thread]]
with over 1.8k upvotes,
despite being *misleading*.

* A better definition of concurrency

Join me now, then, for an eye-opening definition of concurrency presented in [[https://slikts.github.io/concurrency-glossary/][The Glossary]]:
#+begin_quote
Concurrency is about /independent/ computations
that can be executed in an arbitrary order with the same outcome.
The opposite of concurrent is sequential,
meaning that sequential computations depend on being executed step-by-step
to produce correct results.
#+end_quote

/Ahhh/, like a breath of fresh air!

Right off the bat, notice that in this definition,
concurrency has an entirely different essence:
it is *not* about /how/ you execute tasks,
it is about the tasks themselves.
In other words, it is not a /technique/, or a description of a technique,
rather, it is /property/ of tasks in relation to one another.

Thus, it is incorrect to say "executing tasks concurrently".
Instead, we talk about "factoring a program into concurrent modules",
or simply "concurrent programming".

Pay attention to the key term "independent" --
it is actually defined as a synonym for "concurrent"! [fn:6]
That is /awesome/, because "independent tasks"
makes much more intuitive sense than "concurrent tasks".

Note however that concurrent tasks are almost never completely independent,
instead they have some dependencies between each other.
One example of a dependency
is when a task needs to wait for a result from another task to be able to continue.
Exercise for the reader:
try to figure out what is the fundamental nature of dependencies between tasks.

When the interaction between interdependent tasks is well-defined in the model,
it is called /communication/ [fn:4].
The two most popular paradigms for communication are shared memory and message passing.
They don't restrict the programmer,
which means you have the freedom to do anything you want.
But this freedom comes at a cost:
it's very hard to verify that your programs don't contain bugs like data races.
There exist more restrictive paradigms (e.g. CSP, FRP) that make it impossible to have such bugs,
and we should strive to use such paradigms whenever possible.
For more on this, read Van Roy's article.

Quick note:
Most commonly (I think), when we speak of concurrency,
we imagine OS threads,
but there are many other things that can be concurrent:
coroutines, processes, actors, etc.
What umbrella term can we use to capture all these units of concurrency?
Van Roy uses the term "parts", as in "parts of program code".
The Glossary uses the term "unit of concurrency", which is a bit long,
but more frequently it uses "computation",
which I like more because it's more general
(for example, it also applies to the execution of code generated dynamically),
and because in SICP they also talk about "computational processes"
as the fundamental concept of a running program.
So I think "computation" and "computational process" are the best terms to use
to talk about "units of concurrency".
The more colloquial term "task" is probably also fine
because it doesn't have other implementation-specific meanings
(unlike the term "thread" for example).

Now look at this:
#+begin_quote
Concurrent programming means factoring a program into independent modules or units of concurrency.
#+end_quote
I think we should avoid saying "execute X and Y concurrently"
and instead say "write a concurrent program that does X and Y"
to emphasize that "concurrency" is a /property/ of tasks, not a technique.

Now let's continue to parallelism:
#+begin_quote
Parallelism refers to executing multiple computations at the same time, while serial execution is one-at-a-time. Parallelization and serialization refer to composing computations either in parallel or serially.

The colloquial meanings of "concurrent" and "parallel" are largely synonymous, which is a source of significant confusion that extends even to computer science literature, where concurrency may be misleadingly described in terms that imply or explicitly refer to overlapping lifetimes.
#+end_quote

I /love/ it when after long hours of frustration I stumble upon an article
that acknowledges that there is confusion out there and proceeds to sort things out.

Van Roy's article delivers the final blow to put an end to all confusion:
#+begin_quote
Concurrency and parallelism are orthogonal:
it is possible to run concurrent programs on a single processor
(using preemptive scheduling and time slices)
and to run sequential programs on multiple processors
(by parallelizing the calculations).
#+end_quote

So there you go, my old ideas about concurrency implying parallelism
got knocked out the window,
and in its place a beautiful model emerged,
shining with abstract purity and generality.

#+caption: Examples showing that concurrency and parallelism are orthogonal
|                    | Sequential tasks    | Concurrent tasks                   |
|--------------------+---------------------+------------------------------------|
| Serial execution   | regular programming | time slicing on a single processor |
| Parallel execution | [[https://en.wikipedia.org/wiki/Single_instruction,_multiple_data][SIMD]]                | threads running in parallel        |


* Analyzing misleading definitions from the web

Although the new definition of concurrency is fundamentally different,
you can see how most other definitions follow from it. Let's see a few:

#+begin_quote
Concurrency is when two or more tasks can start, run, and complete in overlapping time periods
-- [[https://stackoverflow.com/a/1050257][Top answer on StackOverflow]]
#+end_quote

I believe this definition is equivalent to the one from The Glossary.
Indeed, if two tasks are independent and can be run in any order,
then it follows that they can be sliced and interleaved using the time slicing technique.

However, you can see how it hints heavily at the idea of time slicing,
which might confuse some to believe that concurrency is about time slicing,
whereas in fact the two ideas are orthogonal.
Which is why I don't like it as an introductory definition of concurrency.
I think the relation between concurrency and the technique of time slicing
is better made explicit through a statement like this:
"Concurrent tasks can be executed seemingly in parallel
using the technique of time slicing".

Also, I think it doesn't extend well to capture the idea of partially interdependent tasks.

Here is another one from the same Stack Overflow thread, but written as a comment to the question, which didn't stop it from getting 450 upvotes:
#+begin_quote
short answer: Concurrency is two lines of customers ordering from a single cashier (lines take turns ordering); Parallelism is two lines of customers ordering from two cashiers (each line gets its own cashier).
-- Comment on the same [[https://stackoverflow.com/questions/1050222/what-is-the-difference-between-concurrency-and-parallelism][SO question]]
#+end_quote

This analogy doesn't explain concurrency,
it explains *time slicing*.
Replace the word "concurrent" with "serial, but using time slicing",
and the analogy becomes correct.

Again, concurrency and time slicing certainly are good friends
and often go hand in hand.
But if we want to avoid confusion, we must be rigorous about what is what.

Interestingly, this analogy was presented [[https://joearms.github.io/published/2013-04-05-concurrent-and-parallel-programming.html][on Joe Armstrong's blog]],
one of the fathers of Erlang, a language that is primarily about concurrency.
Granted, it did generate quite a heated conversation,
which might have been Armstrong's goal with this post.

#+caption: Joe Armstrong's coffee machine analogy
[[https://joearms.github.io/images/con_and_par.jpg]]

An improvement on this analogy would be to present people in an unordered crowd
rather than in a queue, as I've tried doing here:
#+caption: My autographs analogy
[[../static/autographs_analogy.png]]

It represents concurrency without parallelism with kids crowded around a table for autographs:
it doesn't matter in what order they go, so they are independent,
and the celebrity can only sign one autograph at a time, so it's serial.
To add parallelism, we simply create a clone of the celebrity.

However, this analogy is not perfect either.
What in this analogy is telling us that the kids standing in the queue are actually interdependent?
Logically speaking, it doesn't matter in what order the kids will get their autographs,
so they are still independent, even though they are standing in a queue[fn:5].

* My tutorial on concurrency

I like the time slicing diagrams,
so I tried try to adapt them to our new definition of concurrency.
In this tutorial, two tasks are sequential if they are locked to each other,
and concurrent otherwise.
Parallelism is self-explanatory.

#+caption: A tutorial on concurrency and parallelism
[[../static/concurrency_tutorial.png]]


* Why are there so many definitions?

One reason why the misleading definitions and analogies exist
is that concurrency is implemented in different ways depending on the level.
For example, in the world of Operating Systems,
concurrency is represented by threads and time slicing.
In the software engineering world, it is represented by coroutines.
Distributed systems need study concurrency more in-depth,
so they will find the correct definition very useful though.

Another reason is that an accurate and general definition of concurrency
(like the one from The Glossary)
might seem a bit too abstract for some people.
They might prefer a simple but imprecise analogy
because it gives them that feeling of intuitive understanding.

However, I believe that such hand-wavy definitions
can only give you a partial understanding at best,
and a completely wrong understanding at worst,
so they are not good enough by themselves
(they can still be used as stepping stones).
Any programmer who is serious about his craft will not stop at flimsy analogies
but will instead keep digging,
progressing from simpler definitions to more rigorous ones,
until he finds the single source of truth
and is able to grasp it and build a correct understanding of the topic.

The Glossary contains the best definitions I've found so far,
but as the author points out, it is just an "informal top-level overview",
so we have to keep digging.

* Footnotes
[fn:6] Indeed, in [[https://www.researchgate.net/publication/241111987][the article]] linked in The Glossary,
"Programming Paradigms for Dummies" by Peter Van Roy
(which is great and you must read! [fn:3]),
"independence" is used as a synonym for "concurrency".
 
[fn:1] Of course, there are no correct definitions of concurrency.
Programmers are still arguing on this decades-old topic for a reason.
However, for myself, I will consider them correct
because they are just so much better than anything else I've found so far.

[fn:2] After all, if a thing *appears* to be a certain way,
then it either *truly* is that way,
or it is not that way,
but we *perceive* it to be that way (because of an illusion or something),
thus "appearing to be a certain way" is a proper superset
of "truly being a certain way". QED.

[fn:3] In fact, the author wrote a book
titled [[https://en.wikipedia.org/wiki/Concepts,_Techniques,_and_Models_of_Computer_Programming]["Concepts, Techniques, and Models of Computer Programming"]]
which from a quick glance is very based in the same way as SICP.
[[https://news.ycombinator.com/item?id=18383531][Looks like]] it puts an end to the dumb wars of paradigms a-la "FP vs. OOP"
by seeing the use-case for each
-- [[https://webperso.info.ucl.ac.be/~pvr/book.html]["More is not better (or worse) than less, just different"]]
(but it agrees that FP should be the default, which makes me fall in love).
Will definitely give it a read.

[fn:4] The opposite of well-defined interaction is probably
unintended interaction (e.g. when the programmer didn't expect that two threads will be writing to the same file).

[fn:5] This could be solved if instead of autographs,
for example, the problem were to rank the kids by height.
If we admit that the processor (the table) is simply assigning a number to each kid
and then incrementing it,
then the order in which the kids came up to the table would indeed matter
(they would have to be sorted by height),
and we could say they are sequential.
But this doesn't sound like a very intuitive analogy.
